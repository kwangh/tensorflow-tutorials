{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn/ptb/reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename,'r') as f:\n",
    "        return f.read().decode('utf-8').replace('\\n','<eos>').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_vocab(filename):\n",
    "    data=_read_words(filename)\n",
    "    \n",
    "    counter=collections.Counter(data)\n",
    "    count_pairs=sorted(counter.items(),key=lambda x:(-x[1],x[0])) # sorted by counts, if equal alphabetical order\n",
    "    \n",
    "    words,_=list(zip(*count_pairs))\n",
    "    word_to_id=dict(zip(words,range(len(words))))\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _file_to_word_ids(filename,word_to_id):\n",
    "    data=_read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path=None):\n",
    "    train_path=os.path.join(data_path,'ptb.train.txt')\n",
    "    valid_path=os.path.join(data_path,'ptb.valid.txt')\n",
    "    test_path=os.path.join(data_path,'ptb.test.txt')\n",
    "    \n",
    "    word_to_id=_build_vocab(train_path)\n",
    "    train_data=_file_to_word_ids(train_path,word_to_id)\n",
    "    valid_data=_file_to_word_ids(valid_path,word_to_id)\n",
    "    test_data=_file_to_word_ids(test_path,word_to_id)\n",
    "    vocabulary=len(word_to_id) # not used\n",
    "    return train_data,valid_data,test_data,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data,batch_size,num_steps,name=None):\n",
    "    with tf.name_scope(name,'PTBProducer',[raw_data,batch_size,num_steps]):\n",
    "        raw_data=tf.convert_to_tensor(raw_data,name='raw_data',dtype=tf.int32)\n",
    "        data_len=tf.size(raw_data)\n",
    "        batch_len=data_len//batch_size\n",
    "        \n",
    "        data=tf.reshape(raw_data[0:batch_size*batch_len],[batch_size,batch_len])\n",
    "        \n",
    "        epoch_size=(batch_len-1) // num_steps\n",
    "        assertion=tf.assert_positive(epoch_size,message='epoch_size == 0,decrease batch_size or num_steps')        \n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size=tf.identity(epoch_size,name='epoch_size')\n",
    "            \n",
    "        i=tf.train.range_input_producer(epoch_size,shuffle=False).dequeue()\n",
    "        x=tf.slice(data,[0,i*num_steps],[batch_size,num_steps])\n",
    "        y=tf.slice(data,[0,i*num_steps+1],[batch_size,num_steps])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# reader_test.py example\n",
    "raw_data = [4, 3, 2, 1, 0, 5, 6, 1, 1, 1, 1, 0, 3, 4, 1]\n",
    "batch_size = 3\n",
    "num_steps = 2\n",
    "\n",
    "with tf.name_scope(None,'PTBProducer',[raw_data,batch_size,num_steps]):\n",
    "    raw_data=tf.convert_to_tensor(raw_data,name='raw_data',dtype=tf.int32)\n",
    "    data_len=tf.size(raw_data)\n",
    "    batch_len=data_len//batch_size\n",
    "    data=tf.reshape(raw_data[0:batch_size*batch_len],[batch_size,batch_len])\n",
    "        \n",
    "    epoch_size=(batch_len-1) // num_steps\n",
    "    assertion=tf.assert_positive(epoch_size,message='epoch_size == 0,decrease batch_size or num_steps')        \n",
    "    with tf.control_dependencies([assertion]):\n",
    "        epoch_size=tf.identity(epoch_size,name='epoch_size')\n",
    "        \n",
    "    i=tf.train.range_input_producer(epoch_size,shuffle=False).dequeue()\n",
    "    x=tf.slice(data,[0,i*num_steps],[batch_size,num_steps])\n",
    "    y=tf.slice(data,[0,i*num_steps+1],[batch_size,num_steps])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    tf.train.start_queue_runners(sess, coord=coord)\n",
    "    try:\n",
    "        print(sess.run([i,epoch_size]))\n",
    "        print(sess.run(i))\n",
    "        print(sess.run(i))\n",
    "        print(sess.run(i))\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ptb_word_lm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags=tf.flags\n",
    "logging=tf.logging\n",
    "\n",
    "flags.DEFINE_string('model','small','A type of model. Possible options are: small, medium, large')\n",
    "flags.DEFINE_string('data_path','ptb','Where the training/test data is stored')\n",
    "flags.DEFINE_string('save_path','ptb','Model output directory')\n",
    "flags.DEFINE_bool('use_fp16',False,'Train using 16-bit floats instead of 32bit floats')\n",
    "\n",
    "FLAGS=flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    init_scale=0.1\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    num_steps=20\n",
    "    hidden_size=200\n",
    "    max_epoch=4\n",
    "    max_max_epoch=13\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.5\n",
    "    batch_size=20\n",
    "    vocab_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MediumConfig(object):\n",
    "    init_scale=0.05\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    num_steps=35\n",
    "    hidden_size=650\n",
    "    max_epoch=6\n",
    "    max_max_epoch=39\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.8\n",
    "    batch_size=20\n",
    "    vocab_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LargeConfig(object):\n",
    "    init_scale=0.04\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=10\n",
    "    num_layers=2\n",
    "    num_steps=35\n",
    "    hidden_size=1500\n",
    "    max_epoch=14\n",
    "    max_max_epoch=55\n",
    "    keep_prob=0.35\n",
    "    lr_decay=1/1.15\n",
    "    batch_size=20\n",
    "    vocab_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestConfig(object):\n",
    "    'Tiny config, for testing.'\n",
    "    init_scale=0.1\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=1\n",
    "    num_layers=1\n",
    "    num_steps=2\n",
    "    hidden_size=2\n",
    "    max_epoch=1\n",
    "    max_max_epoch=1\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.5\n",
    "    batch_size=20\n",
    "    vocab_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    if FLAGS.model=='small':\n",
    "        return SmallConfig()\n",
    "    elif FLAGS.model=='medium':\n",
    "        return MediumConfig()\n",
    "    elif FLAGS.model=='large':\n",
    "        return LargeConfig()\n",
    "    elif FLAGS.model=='test':\n",
    "        return TestConfig()\n",
    "    else:\n",
    "        raise ValueError('Invalid model: %s',FLAGS.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float16 if FLAGS.use_fp16 else tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    def __init__(self,config,data,name=None):\n",
    "        self.batch_size=batch_size=config.batch_size\n",
    "        self.num_steps=num_steps=config.num_steps\n",
    "        self.epoch_size=((len(data)//batch_size)-1)//num_steps\n",
    "        self.input_data,self.targets=ptb_producer(data,batch_size,num_steps,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self,is_training,config,input_):\n",
    "        self._input=input_\n",
    "        \n",
    "        batch_size=input_.batch_size\n",
    "        num_steps=input_.num_steps\n",
    "        size=config.hidden_size\n",
    "        vocab_size=config.vocab_size\n",
    "        \n",
    "        lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(size,forget_bias=0.0,state_is_tuple=True)\n",
    "        if is_training and config.keep_prob<1:\n",
    "            lstm_cell=tf.nn.rnn_cell.DropoutWrapper(lstm_cell,output_keep_prob=config.keep_prob)\n",
    "        cell=tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*config.num_layers,state_is_tuple=True)\n",
    "        \n",
    "        self._initial_state=cell.zero_state(batch_size,data_type())\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding=tf.get_variable('embedding',[vocab_size,size],dtype=data_type())\n",
    "            inputs=tf.nn.embedding_lookup(embedding,input_.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob<1:\n",
    "            inputs=tf.nn.droput(inputs,config.keep_prob)\n",
    "            \n",
    "        outputs=[]\n",
    "        state=self._initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step>0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output,state)=cell(inputs[:,time_step,:],state)\n",
    "                outputs.append(cell_output)\n",
    "                \n",
    "        output=tf.reshape(tf.concat(1,outputs),[-1,size])\n",
    "        softmax_w=tf.get_variable('softmax_w',[size,vocab_size],dtype=data_type())\n",
    "        softmax_b=tf.get_variable('softmax_b',[vocab_size],dtype=data_type())\n",
    "        logits=tf.matmul(output,softmax_w)+softmax_b\n",
    "        loss=tf.nn.seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(input_.targets,[-1])],\n",
    "        [tf.ones([batch_size*num_steps],dtype=data_type())])\n",
    "        self._cost=cost=tf.reduce_sum(loss)/batch_size\n",
    "        self._final_state=state\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        self._lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op=optimizer.apply_gradients(\n",
    "            zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        self._new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        self._lr_update=tf.assign(self._lr,self._new_lr)\n",
    "        \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(self._lr_update,feed_dict={self._new_lr:lr_value})\n",
    "\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session,model,eval_op=None,verbose=False):\n",
    "    start_time=time.time()\n",
    "    costs=0.0\n",
    "    iters=0\n",
    "    state=session.run(model.initial_state())\n",
    "    \n",
    "    fetches={\n",
    "        'cost':model.cost(),\n",
    "        'final_state':model.final_state(),\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches['eval_op']=eval_op\n",
    "        \n",
    "    for step in range(model.input().epoch_size):\n",
    "        feed_dict={}\n",
    "        for i,(c,h) in enumerate(model.initial_state()):\n",
    "            feed_dict[c]=state[i].c\n",
    "            feed_dict[h]=state[i].h\n",
    "            \n",
    "        vals=session.run(fetches,feed_dict)\n",
    "        cost=vals['cost']\n",
    "        state=vals['final_state']\n",
    "        \n",
    "        costs+=cost\n",
    "        iters+=model.input().num_steps\n",
    "        \n",
    "        if verbose and step % (model.input().epoch_size//10)==10:\n",
    "            print('%.3f perplexity: %.3f speed: %.0f wps' % \n",
    "                  (step*1.0/model.input().epoch_size,np.exp(costs/iters),\n",
    "                  iters*model.input().batch_size/(time.time()-start_time)))\n",
    "    return np.exp(costs/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not FLAGS.data_path:\n",
    "    raise ValueError('Must set --data_path to PTB data directory')\n",
    "    \n",
    "train_data,valid_data,test_data,_=ptb_raw_data(FLAGS.data_path)\n",
    "\n",
    "config=get_config()\n",
    "eval_config=get_config()\n",
    "eval_config.batch_size=1\n",
    "eval_config.num_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning rate: 1.000\n",
      "0.004 perplexity: 5965.121 speed: 5273 wps\n",
      "0.104 perplexity: 836.646 speed: 18855 wps\n",
      "0.204 perplexity: 621.997 speed: 21972 wps\n",
      "0.304 perplexity: 503.289 speed: 23343 wps\n",
      "0.404 perplexity: 435.003 speed: 24112 wps\n",
      "0.504 perplexity: 390.037 speed: 24584 wps\n",
      "0.604 perplexity: 351.578 speed: 24928 wps\n",
      "0.703 perplexity: 324.803 speed: 25123 wps\n",
      "0.803 perplexity: 303.517 speed: 25290 wps\n",
      "0.903 perplexity: 283.973 speed: 25416 wps\n",
      "Epoch: 1 Train Perplexity: 269.580\n",
      "Epoch: 1 Valid Perplexity: 182.811\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.004 perplexity: 202.417 speed: 27499 wps\n",
      "0.104 perplexity: 150.811 speed: 26576 wps\n",
      "0.204 perplexity: 158.375 speed: 26514 wps\n",
      "0.304 perplexity: 152.989 speed: 26607 wps\n",
      "0.404 perplexity: 150.200 speed: 26525 wps\n",
      "0.504 perplexity: 147.834 speed: 26521 wps\n",
      "0.604 perplexity: 143.278 speed: 26581 wps\n",
      "0.703 perplexity: 141.084 speed: 26590 wps\n",
      "0.803 perplexity: 139.063 speed: 26621 wps\n",
      "0.903 perplexity: 135.405 speed: 26635 wps\n",
      "Epoch: 2 Train Perplexity: 133.460\n",
      "Epoch: 2 Valid Perplexity: 147.009\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.004 perplexity: 140.672 speed: 27274 wps\n",
      "0.104 perplexity: 105.100 speed: 26427 wps\n",
      "0.204 perplexity: 114.505 speed: 26646 wps\n",
      "0.304 perplexity: 111.451 speed: 26661 wps\n",
      "0.404 perplexity: 110.689 speed: 26693 wps\n",
      "0.504 perplexity: 109.872 speed: 26635 wps\n",
      "0.604 perplexity: 107.234 speed: 26663 wps\n",
      "0.703 perplexity: 106.555 speed: 26640 wps\n",
      "0.803 perplexity: 105.946 speed: 26623 wps\n",
      "0.903 perplexity: 103.669 speed: 26633 wps\n",
      "Epoch: 3 Train Perplexity: 102.786\n",
      "Epoch: 3 Valid Perplexity: 135.569\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.004 perplexity: 113.354 speed: 27157 wps\n",
      "0.104 perplexity: 85.301 speed: 26261 wps\n",
      "0.204 perplexity: 94.035 speed: 26220 wps\n",
      "0.304 perplexity: 91.604 speed: 26390 wps\n",
      "0.404 perplexity: 91.404 speed: 26361 wps\n",
      "0.504 perplexity: 90.955 speed: 26354 wps\n",
      "0.604 perplexity: 89.085 speed: 26366 wps\n",
      "0.703 perplexity: 88.846 speed: 26438 wps\n",
      "0.803 perplexity: 88.626 speed: 26427 wps\n",
      "0.903 perplexity: 86.961 speed: 26431 wps\n",
      "Epoch: 4 Train Perplexity: 86.454\n",
      "Epoch: 4 Valid Perplexity: 128.564\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "0.004 perplexity: 98.106 speed: 27929 wps\n",
      "0.104 perplexity: 71.634 speed: 26896 wps\n",
      "0.204 perplexity: 77.863 speed: 26636 wps\n",
      "0.304 perplexity: 74.728 speed: 26544 wps\n",
      "0.404 perplexity: 73.832 speed: 26550 wps\n",
      "0.504 perplexity: 72.758 speed: 26525 wps\n",
      "0.604 perplexity: 70.619 speed: 26549 wps\n",
      "0.703 perplexity: 69.823 speed: 26481 wps\n",
      "0.803 perplexity: 69.034 speed: 26490 wps\n",
      "0.903 perplexity: 67.093 speed: 26506 wps\n",
      "Epoch: 5 Train Perplexity: 66.101\n",
      "Epoch: 5 Valid Perplexity: 119.046\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "0.004 perplexity: 79.431 speed: 27655 wps\n",
      "0.104 perplexity: 58.715 speed: 26670 wps\n",
      "0.204 perplexity: 64.135 speed: 26398 wps\n",
      "0.304 perplexity: 61.565 speed: 26416 wps\n",
      "0.404 perplexity: 60.839 speed: 26462 wps\n",
      "0.504 perplexity: 59.904 speed: 26434 wps\n",
      "0.604 perplexity: 58.046 speed: 26432 wps\n",
      "0.703 perplexity: 57.296 speed: 26469 wps\n",
      "0.803 perplexity: 56.507 speed: 26494 wps\n",
      "0.903 perplexity: 54.754 speed: 26506 wps\n",
      "Epoch: 6 Train Perplexity: 53.793\n",
      "Epoch: 6 Valid Perplexity: 118.378\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "0.004 perplexity: 69.958 speed: 27517 wps\n",
      "0.104 perplexity: 52.070 speed: 25870 wps\n",
      "0.204 perplexity: 57.011 speed: 26244 wps\n",
      "0.304 perplexity: 54.692 speed: 26315 wps\n",
      "0.404 perplexity: 54.047 speed: 26254 wps\n",
      "0.504 perplexity: 53.170 speed: 26146 wps\n",
      "0.604 perplexity: 51.475 speed: 26005 wps\n",
      "0.703 perplexity: 50.789 speed: 26109 wps\n",
      "0.803 perplexity: 49.995 speed: 26133 wps\n",
      "0.903 perplexity: 48.387 speed: 26165 wps\n",
      "Epoch: 7 Train Perplexity: 47.479\n",
      "Epoch: 7 Valid Perplexity: 119.488\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "0.004 perplexity: 63.012 speed: 27312 wps\n",
      "0.104 perplexity: 48.598 speed: 26003 wps\n",
      "0.204 perplexity: 53.324 speed: 26429 wps\n",
      "0.304 perplexity: 51.125 speed: 26318 wps\n",
      "0.404 perplexity: 50.544 speed: 26357 wps\n",
      "0.504 perplexity: 49.679 speed: 26356 wps\n",
      "0.604 perplexity: 48.093 speed: 26382 wps\n",
      "0.703 perplexity: 47.438 speed: 26385 wps\n",
      "0.803 perplexity: 46.659 speed: 26386 wps\n",
      "0.903 perplexity: 45.127 speed: 26395 wps\n",
      "Epoch: 8 Train Perplexity: 44.245\n",
      "Epoch: 8 Valid Perplexity: 120.466\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "0.004 perplexity: 60.603 speed: 26429 wps\n",
      "0.104 perplexity: 46.834 speed: 26384 wps\n",
      "0.204 perplexity: 51.456 speed: 26398 wps\n",
      "0.304 perplexity: 49.305 speed: 26350 wps\n",
      "0.404 perplexity: 48.734 speed: 26355 wps\n",
      "0.504 perplexity: 47.893 speed: 26351 wps\n",
      "0.604 perplexity: 46.351 speed: 26356 wps\n",
      "0.703 perplexity: 45.705 speed: 26366 wps\n",
      "0.803 perplexity: 44.934 speed: 26339 wps\n",
      "0.903 perplexity: 43.436 speed: 26339 wps\n",
      "Epoch: 9 Train Perplexity: 42.566\n",
      "Epoch: 9 Valid Perplexity: 120.706\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "0.004 perplexity: 59.377 speed: 26730 wps\n",
      "0.104 perplexity: 45.861 speed: 26414 wps\n",
      "0.204 perplexity: 50.433 speed: 26348 wps\n",
      "0.304 perplexity: 48.316 speed: 26362 wps\n",
      "0.404 perplexity: 47.755 speed: 26361 wps\n",
      "0.504 perplexity: 46.931 speed: 26359 wps\n",
      "0.604 perplexity: 45.414 speed: 26355 wps\n",
      "0.703 perplexity: 44.770 speed: 26363 wps\n",
      "0.803 perplexity: 44.003 speed: 26362 wps\n",
      "0.903 perplexity: 42.522 speed: 26353 wps\n",
      "Epoch: 10 Train Perplexity: 41.685\n",
      "Epoch: 10 Valid Perplexity: 120.464\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "0.004 perplexity: 58.687 speed: 26725 wps\n",
      "0.104 perplexity: 45.168 speed: 26313 wps\n",
      "0.204 perplexity: 49.799 speed: 26343 wps\n",
      "0.304 perplexity: 47.711 speed: 26331 wps\n",
      "0.404 perplexity: 47.205 speed: 26357 wps\n",
      "0.504 perplexity: 46.361 speed: 26361 wps\n",
      "0.604 perplexity: 44.881 speed: 26371 wps\n",
      "0.703 perplexity: 44.236 speed: 26369 wps\n",
      "0.803 perplexity: 43.454 speed: 26372 wps\n",
      "0.903 perplexity: 42.003 speed: 26361 wps\n",
      "Epoch: 11 Train Perplexity: 41.159\n",
      "Epoch: 11 Valid Perplexity: 120.155\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "0.004 perplexity: 58.264 speed: 27025 wps\n",
      "0.104 perplexity: 44.826 speed: 26397 wps\n",
      "0.204 perplexity: 49.442 speed: 26383 wps\n",
      "0.304 perplexity: 47.382 speed: 26385 wps\n",
      "0.404 perplexity: 46.888 speed: 26392 wps\n",
      "0.504 perplexity: 46.054 speed: 26395 wps\n",
      "0.604 perplexity: 44.586 speed: 26390 wps\n",
      "0.703 perplexity: 43.944 speed: 26381 wps\n",
      "0.803 perplexity: 43.167 speed: 26393 wps\n",
      "0.903 perplexity: 41.723 speed: 26401 wps\n",
      "Epoch: 12 Train Perplexity: 40.883\n",
      "Epoch: 12 Valid Perplexity: 119.871\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "0.004 perplexity: 58.018 speed: 26754 wps\n",
      "0.104 perplexity: 44.628 speed: 26372 wps\n",
      "0.204 perplexity: 49.236 speed: 26312 wps\n",
      "0.304 perplexity: 47.196 speed: 26312 wps\n",
      "0.404 perplexity: 46.710 speed: 26281 wps\n",
      "0.504 perplexity: 45.884 speed: 26348 wps\n",
      "0.604 perplexity: 44.424 speed: 26419 wps\n",
      "0.703 perplexity: 43.785 speed: 26434 wps\n",
      "0.803 perplexity: 43.011 speed: 26447 wps\n",
      "0.903 perplexity: 41.572 speed: 26445 wps\n",
      "Epoch: 13 Train Perplexity: 40.735\n",
      "Epoch: 13 Valid Perplexity: 119.702\n",
      "Test Perplexity: 114.676\n",
      "Saving model to ptb.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "    with tf.name_scope('Train'):\n",
    "        train_input=PTBInput(config=config,data=train_data,name='TrainInput')\n",
    "        with tf.variable_scope('Model',reuse=None,initializer=initializer):\n",
    "            m=PTBModel(is_training=True,config=config,input_=train_input)\n",
    "        tf.scalar_summary('Training Loss',m.cost())\n",
    "        tf.scalar_summary('Learning Rate',m.lr())\n",
    "        \n",
    "    with tf.name_scope('Valid'):\n",
    "        valid_input=PTBInput(config=config,data=valid_data,name='ValidInput')\n",
    "        with tf.variable_scope('Model',reuse=True,initializer=initializer):\n",
    "            mvalid=PTBModel(is_training=False,config=config,input_=valid_input)\n",
    "        tf.scalar_summary('Validation Loss',mvalid.cost())\n",
    "        \n",
    "    with tf.name_scope('Test'):\n",
    "        test_input=PTBInput(config=eval_config,data=test_data,name='TestInput')\n",
    "        with tf.variable_scope('Model',reuse=True,initializer=initializer):\n",
    "            mtest=PTBModel(is_training=False,config=eval_config,input_=test_input)\n",
    "        \n",
    "    sv=tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay=config.lr_decay**max(i+1-config.max_epoch,0.0)\n",
    "            m.assign_lr(session,config.learning_rate*lr_decay)\n",
    "            \n",
    "            print('Epoch: %d Learning rate: %.3f' % (i+1,session.run(m.lr())))\n",
    "            train_perplexity=run_epoch(session,m,eval_op=m.train_op(),verbose=True)\n",
    "            \n",
    "            print('Epoch: %d Train Perplexity: %.3f' % (i+1,train_perplexity))\n",
    "            valid_perplexity=run_epoch(session,mvalid)\n",
    "            print('Epoch: %d Valid Perplexity: %.3f' % (i+1,valid_perplexity))\n",
    "            \n",
    "        test_perplexity=run_epoch(session,mtest)\n",
    "        print('Test Perplexity: %.3f' % test_perplexity)\n",
    "        \n",
    "        if FLAGS.save_path:\n",
    "            print('Saving model to %s.' % FLAGS.save_path)\n",
    "            sv.saver.save(session,FLAGS.save_path,global_step=sv.global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

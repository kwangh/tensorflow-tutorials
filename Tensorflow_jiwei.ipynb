{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "data_path=SICK\n",
      "embedding_dim=300\n",
      "word2vec=embeddings/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec', 'embeddings/GoogleNews-vectors-negative300.bin', 'Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','SICK','SICK data set path')\n",
    "flags.DEFINE_integer('embedding_dim', 300, 'Dimensionality of word embedding')\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path=None):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec file {}'.format(word2vec_path))\n",
    "        with open(FLAGS.word2vec,'rb') as f:\n",
    "            header=f.readline()\n",
    "            vocab_size,layer2_size=map(int,header.split())\n",
    "            # initial matrix with random uniform\n",
    "            init_W=np.random.uniform(-0.25,0.25,(vocab_size,FLAGS.embedding_dim))\n",
    "\n",
    "            binary_len=np.dtype('float32').itemsize*FLAGS.embedding_dim\n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            dictionary=dict()\n",
    "            for line in xrange(vocab_size):\n",
    "                word=[]\n",
    "                while True:\n",
    "                    ch=f.read(1)\n",
    "                    if ch==' ':\n",
    "                        word=''.join(word)\n",
    "                        break\n",
    "                    if ch!='\\n':\n",
    "                        word.append(ch)\n",
    "                word=word.decode('utf-8')\n",
    "                dictionary[word]=len(dictionary)\n",
    "                init_W[dictionary[word]]=np.fromstring(f.read(binary_len),dtype='float32')\n",
    "\n",
    "            #reverse_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "        #return dictionary,reverse_dictionary,init_W\n",
    "        return dictionary,init_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename,word_to_id):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            pair_ID=line.split('\\t')[0] # for trial & test\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]\n",
    "            sentences_A.append([word_to_id[word] for word in sentence_A.split() if word in word_to_id])\n",
    "            sentencesA_length.append(len(sentence_A.split()))\n",
    "            sentences_B.append([word_to_id[word] for word in sentence_B.split() if word in word_to_id])\n",
    "            sentencesB_length.append(len(sentence_B.split()))\n",
    "            relatedness_scores.append(relatedness_score)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSInput(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores=file_to_word_ids(train_path,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_path=os.path.join(FLAGS.data_path,'SICK_train.txt')\n",
    "valid_path=os.path.join(FLAGS.data_path,'SICK_trial.txt')\n",
    "test_path=os.path.join(FLAGS.data_path,'SICK_test.txt')\n",
    "\n",
    "dictionary,init_W=build_vocab(FLAGS.word2vec)\n",
    "train_data=file_to_word_ids(train_path,dictionary)\n",
    "valid_data=file_to_word_ids(valid_path,dictionary)\n",
    "test_data=file_to_word_ids(test_path,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(start,end,input):\n",
    "    inputs_A=input.sentences_A[start:end]\n",
    "    inputsA_length=input.sentencesA_length[start:end]\n",
    "    inputs_B=input.sentences_B[start:end]\n",
    "    inputsB_length=input.sentencesB_length[start:end]\n",
    "    labels=np.reshape(input.relatedness_scores[start:end],(len(range(start,end)),1))\n",
    "    return inputs_A,inputsA_length,inputs_B,inputsB_length,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainConfig(object):\n",
    "    init_scale=0.01\n",
    "    learning_rate=0.01\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.5\n",
    "    batch_size=20\n",
    "    \n",
    "class TestConfig(object):\n",
    "    init_sclae=0.1\n",
    "    learning_rate=0.01\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.5\n",
    "    batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSModel(object):\n",
    "    def __init__(self,is_training,config):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

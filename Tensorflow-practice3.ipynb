{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url='http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename,_=urllib.request.urlretrieve(url+filename,filename)\n",
    "    statinfo=os.stat(filename)\n",
    "    if statinfo.st_size==expected_bytes:\n",
    "        print('Found and verified',filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify '+filename+'. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename=maybe_download('text8.zip',31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data=tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words=read_data(filename)\n",
    "print('Data size',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size=50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count=[['UNK',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    dictionary=dict()\n",
    "    for word,_ in count:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    data=list()\n",
    "    unk_count=0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index=dictionary[word]\n",
    "        else:\n",
    "            index=0 # dictionary['UNK']\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0][1]=unk_count\n",
    "    reverse_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    return data,count,dictionary,reverse_dictionary\n",
    "\n",
    "data,count,dictionary,reverse_dictionary=build_dataset(words)\n",
    "del words # Hint to reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)',count[:5])\n",
    "print('Sample data',data[:10],[reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084 originated -> 5239 anarchism\n",
      "3084 originated -> 12 as\n",
      "12 as -> 3084 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "data_index=0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips==0 # batchëŠ” 1* num_skips, 2 * num_skips, 3 * ...\n",
    "    assert num_skips<=2*skip_window\n",
    "    batch=np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    span=2*skip_window+1 # [ skip_window target skip_window ]\n",
    "    buffer=collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    for i in range(batch_size//num_skips):\n",
    "        target=skip_window # center index\n",
    "        targets_to_avoid=[skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target=random.randint(0,span-1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i*num_skips+j]=buffer[skip_window]\n",
    "            labels[i*num_skips+j,0]=buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    return batch,labels\n",
    "\n",
    "batch,labels=generate_batch(batch_size=8,num_skips=2,skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i],reverse_dictionary[batch[i]],'->',labels[i,0],reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "embedding_size=128\n",
    "skip_window=1\n",
    "num_skips=2\n",
    "\n",
    "valid_size=16\n",
    "valid_window=100\n",
    "valid_examples=np.random.choice(valid_window,valid_size,replace=False)\n",
    "num_sampled=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs=tf.placeholder(tf.int32,shape=[batch_size])\n",
    "    train_labels=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "    valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "        embed=tf.nn.embedding_lookup(embeddings,train_inputs)\n",
    "        \n",
    "        nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/math.sqrt(embedding_size)))\n",
    "        nce_biases=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    loss=tf.reduce_mean(tf.nn.nce_loss(nce_weights,nce_biases,embed,train_labels,num_sampled,vocabulary_size))\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "    \n",
    "    norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True)) # 0, 1 each column-wise, row-wise reduction, in this case row-wise\n",
    "    normalized_embeddings=embeddings/norm\n",
    "    valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "    similarity=tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)\n",
    "    \n",
    "    init=tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  277.642852783\n",
      "Nearest to into: numb, grown, cond, mindanao, hmos, auteur, photosynthetic, hurl,\n",
      "Nearest to used: mascarene, cretians, migrations, daniels, pics, prosecuted, violates, memorably,\n",
      "Nearest to zero: momenta, adenylate, lutetium, meditator, leslie, obnoxious, predicted, quiz,\n",
      "Nearest to can: nicknamed, corned, undoubtedly, freude, immunoglobulins, testaments, reis, geneticists,\n",
      "Nearest to after: avian, breaching, microcomputer, goal, enumerative, piedmont, gospel, inanimate,\n",
      "Nearest to of: stage, vijay, kush, dictatorships, pacemaker, masorti, bowls, bluish,\n",
      "Nearest to s: euphemistic, garage, bog, arminian, fairness, elbing, semiology, alans,\n",
      "Nearest to american: ornamented, nakano, benediction, escalating, fashions, hak, amphitryon, pantomime,\n",
      "Nearest to about: sinusoidal, lanterns, equations, portraiture, coworkers, dispose, hyperthyroidism, emulator,\n",
      "Nearest to system: rhotic, discovers, sweat, glycoproteins, hummer, oysters, kiesinger, homoerotic,\n",
      "Nearest to two: frontman, orphans, wallet, fulham, paged, ethertype, bridgewater, dll,\n",
      "Nearest to state: memo, barreled, gelfand, viollet, boehm, riff, garde, consultation,\n",
      "Nearest to six: useless, infirmities, minoan, macroevolution, voluminous, transcendentalism, cobbler, dislocation,\n",
      "Nearest to may: aesthetic, incremental, athelstan, methodology, realizes, wields, kaf, languished,\n",
      "Nearest to th: arid, gladiators, shoah, arbitrate, branagh, salieri, pigment, pollinator,\n",
      "Nearest to united: duesberg, capensis, altenberg, dont, beheading, ostpolitik, coax, levinson,\n",
      "Average loss at step  2000 :  113.579761342\n",
      "Average loss at step  4000 :  52.4043413353\n",
      "Average loss at step  6000 :  33.100320029\n",
      "Average loss at step  8000 :  23.6699030712\n",
      "Average loss at step  10000 :  17.9504000062\n",
      "Nearest to into: under, scenarios, omnibus, milne, and, auteur, smell, gave,\n",
      "Nearest to used: reginae, migrations, daniels, paintings, gold, durable, limited, abolished,\n",
      "Nearest to zero: nine, victoriae, cl, austin, reginae, six, mathbf, vs,\n",
      "Nearest to can: valid, zero, thrown, located, nicknamed, to, ft, cc,\n",
      "Nearest to after: and, goal, heavier, gollancz, apr, adam, circumcision, great,\n",
      "Nearest to of: in, and, victoriae, for, succeeds, with, basins, s,\n",
      "Nearest to s: and, the, vs, of, roper, his, zero, UNK,\n",
      "Nearest to american: benediction, east, and, ram, altruist, faber, write, league,\n",
      "Nearest to about: sinusoidal, equations, massive, proclamation, dispose, austin, emulator, forbidden,\n",
      "Nearest to system: philosopher, discovers, main, victoriae, mysore, altenberg, provide, end,\n",
      "Nearest to two: one, reginae, victoriae, phi, vs, five, austin, nine,\n",
      "Nearest to state: victoriae, karts, vs, salad, possess, cc, garde, mounted,\n",
      "Nearest to six: eight, zero, reginae, nine, vs, one, cl, victoriae,\n",
      "Nearest to may: victoriae, aesthetic, juneau, phi, education, methodology, pleads, deity,\n",
      "Nearest to th: cl, arid, three, one, hospitals, gladiators, phi, breaking,\n",
      "Nearest to united: altenberg, basins, victoriae, consisting, reginae, nage, familial, vila,\n"
     ]
    }
   ],
   "source": [
    "num_steps=10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss=0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs,batch_labels=generate_batch(batch_size,num_skips,skip_window)\n",
    "        feed_dict={train_inputs:batch_inputs,train_labels:batch_labels}\n",
    "        \n",
    "        _,loss_val=session.run([optimizer,loss],feed_dict=feed_dict)\n",
    "        average_loss+=loss_val\n",
    "        \n",
    "        if step%2000==0:\n",
    "            if step>0:\n",
    "                average_loss/=2000\n",
    "            print('Average loss at step ',step,': ',average_loss)\n",
    "            average_loss=0\n",
    "        \n",
    "        if step%10000==0:\n",
    "            sim=similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word=reverse_dictionary[valid_examples[i]]\n",
    "                top_k=8\n",
    "                nearest=(-sim[i,:]).argsort()[1:top_k+1]\n",
    "                log_str='Nearest to %s:'%valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word=reverse_dictionary[nearest[k]]\n",
    "                    log_str='%s %s,'%(log_str,close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings=normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs,labels,filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0]>=len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18,18)) # in inches\n",
    "    for i,label in enumerate(labels):\n",
    "        x,y=low_dim_embs[i,:]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(label,\n",
    "                    xy=(x,y),\n",
    "                    xytext=(5,2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    tsne=TSNE(perplexity=30,n_components=2,init='pca',n_iter=5000)\n",
    "    plot_only=500\n",
    "    low_dim_embs=tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "    labels=[reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs,labels)\n",
    "    \n",
    "except ImportError:\n",
    "    print('Please install sklearn, matplotlib, and scipy to visualize embeddings.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
